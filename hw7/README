
======================================
600.233: Computer Systems Fundamentals
Joon Hyuck Choi, Neha Kulkarni
JHED ID: jchoi100, nkulkar5
Email: jchoi100@jhu.edu / nkulkar5@jhu.edu
Assignment 7. The Power of Caches
======================================

*********************************************
		PROBLEM 1
*********************************************

Experiment I: Changing Matrix Implementations

When matrix implementations were changed, based on the 
original size of 4096 produced the following results:

[i][j] implementation

Time: 0.037u 0.010s 0:00.04 100.0%	0+0k 0+0io 0pf+0w

==7811== Cachegrind, a cache and branch-prediction profiler
==7811== Copyright (C) 2002-2015, and GNU GPL'd, by Nicholas 
==7811== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright
==7811== Command: ./matrix
==7811== 
--7811-- warning: L3 cache found, using its data for the LL simulation.
==7811== 
==7811== I   refs:      167,898,754
==7811== I1  misses:            673
==7811== LLi misses:            667
==7811== I1  miss rate:        0.00%
==7811== LLi miss rate:        0.00%
==7811== 
==7811== D   refs:       83,942,797  (67,147,709 rd   + 16,795,088 wr)
==7811== D1  misses:      1,050,349  (     1,270 rd   +  1,049,079 wr)
==7811== LLd misses:      1,050,177  (     1,121 rd   +  1,049,056 wr)
==7811== D1  miss rate:         1.3% (       0.0%     +        6.2%  )
==7811== LLd miss rate:         1.3% (       0.0%     +        6.2%  )
==7811== 
==7811== LL refs:         1,051,022  (     1,943 rd   +  1,049,079 wr)
==7811== LL misses:       1,050,844  (     1,788 rd   +  1,049,056 wr)
==7811== LL miss rate:          0.4% (       0.0%     +        6.2%  )
==7811== 
==7811== Branches:       16,803,876  (16,803,653 cond +        223 ind)
==7811== Mispredicts:         5,982  (     5,921 cond +         61 ind)
==7811== Mispred rate:          0.0% (       0.0%     +       27.4%   )

[j][i] implementation

0.235u 0.014s 0:00.25 96.0%	0+0k 0+0io 0pf+0w

==13442== Cachegrind, a cache and branch-prediction profiler
==13442== Copyright (C) 2002-2015, and GNU GPL'd, by Nicholas
==13442== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright
==13442== Command: ./matrix
==13442== 
--13442-- warning: L3 cache found, using its data for the LL simulation.
==13442== 
==13442== I   refs:      167,898,754
==13442== I1  misses:            673
==13442== LLi misses:            667
==13442== I1  miss rate:        0.00%
==13442== LLi miss rate:        0.00%
==13442== 
==13442== D   refs:       83,942,797  (67,147,709 rd   + 16,795,088 wr)
==13442== D1  misses:     16,778,988  (     1,270 rd   + 16,777,718 wr)
==13442== LLd misses:     16,778,816  (     1,121 rd   + 16,777,695 wr)
==13442== D1  miss rate:        20.0% (       0.0%     +       99.9%  )
==13442== LLd miss rate:        20.0% (       0.0%     +       99.9%  )
==13442== 
==13442== LL refs:        16,779,661  (     1,943 rd   + 16,777,718 wr)
==13442== LL misses:      16,779,483  (     1,788 rd   + 16,777,695 wr)
==13442== LL miss rate:          6.7% (       0.0%     +       99.9%  )
==13442== 
==13442== Branches:       16,803,876  (16,803,653 cond +        223 ind)
==13442== Mispredicts:         5,982  (     5,921 cond +         61 ind)
==13442== Mispred rate:          0.0% (       0.0%     +       27.4%   )

0.246u 0.013s 0:00.26 96.1%	0+0k 0+0io 0pf+0w


Analyzing the above results, the [i][j] setup produces 
far faster runtimes than the [j][i] setup. For example, 
the D1 miss rate of the [i][j] setup is only 1.3%, 
which shoots up to 19.9% in the [j][i] setup. 
This is likely because the latter implementation 
needs to fetch more data from memory, 
requiring more cycles and therefore, more time. 
Given spatial locality, when the program accesses a 
given location, spatial locality states that there’s a 
high probability the item next to it will also be accessed. 
As such, with the first implementation, likely the entire 
block of memory is copied over, so the information to run 
from [i][j], [i][j+1] and so on is already in the cache and 
allows for quick access. This is known as sequential memory 
access and is far faster than random memory access, which 
is more likely to occur in the [j][i] implementation, 
where the addresses are jumping around memory and the 
cache has to consistently go back to memory, 
requiring more time.

Experiment II. Altering Size Constants

For this experiment, size constants were varied between 
size 8 and size 8192, receiving the following results: 

[i][j] Implementation

SIZE | Time (sec) | D1 miss rate (%) | Branch MisPredict (%)
------------------------------------------------------------
  32 |    0.00    |       3.8        |        9.8 
 128 |    0.00    |       2.2        |        5.8
1024 |    0.01    |       1.2        |        0.2
2048 |    0.01    |       1.2        |        0.0
4096 |    0.12    |       1.2        |        0.0
8192 |    0.38    |       1.2        |        0.0

[j][i] Implementation

 SIZE | Time (sec) | D1 miss rate (%) | Branch MisPredict (%)
        ------------------------------------------------------
  32 |    0.00    |       3.8        |        9.8 
 128 |    0.00    |      14.8        |        5.8
1024 |    0.04    |      19.8        |        0.2 
2048 |    0.13    |      19.9        |        0.0
4096 |    1.23    |      19.9        |        0.0  
8192 |    7.21    |      19.9        |        0.0

Analysis:

We can see with very small sizes, like size 32, the run time, 
the D1 miss rate, and the branch misprediction rate were all 
consistent despite matrix implementation. As such, very small 
sizes and different matrix implementations don’t differ too much, 
particularly in runtime. On average, it seems that the second 
program takes longer to run than the first program. From size 
1024 onwards, we see that the % miss rate levels out as 
matrix[j+1][i] is likely not included when matrix[j][i] is 
placed into the cache. As size increases, the number of branch 
mispredicts go up, the % goes down because the number of branches 
quickly outscale the number of misdirects.

Experiment III: 

First, the effect of optimization was tested when the size of 
the matrix was small in both cases. Specifically, optimization 
was first tested with a matrix of size 128 on both the [i][j] 
setup and the [j][i] setup. From a time perspective, it’s 
clear that given the fact that the time was already incredibly 
fast, optimization levels had marginal, if any, impact on the 
running of the programs. Specifically, we see that the [i][j] 
setup produces the following results at size 128: 

O1 optimization: 0.000u 0.000s 0:00.00 0.0%	0+0k 0+0io 0pf+0w
O2 optimization: 0.000u 0.000s 0:00.00 0.0%	0+0k 0+0io 0pf+0w
O3 optimization: 0.000u 0.000s 0:00.00 0.0%	0+0k 0+0io 0pf+0w

While the  [ j ] [ i ] setup produces the following results at 
size 128: 

O1 optimization: 0.000u 0.000s 0:00.00 0.0%	0+0k 0+0io 0pf+0w
O2 optimization: 0.000u 0.000s 0:00.00 0.0%	0+0k 0+0io 0pf+0w
O3 optimization: 0.000u 0.000s 0:00.00 0.0%	0+0k 0+0io 0pf+0w


Clearly the size effect dominates the optimization effect; given 
that the matrix size is small enough, that both setups produce the 
same runtimes. This is likely because the block of memory that 
has been transferred from RAM to cachce encompasses the entire 
span of the matrix; thus, even though the [j] [i] matrix IS jumping 
around, it’s doing so in the cache itself, still maintaining cache 
speed. As such, it’s clear the optimization doesn’t have much of an 
effect on the speed - given a size this small. Analyzing the 
Valgrind data for this size we see: 

 [ i ] [ j ] setup 
** O0 optimization **
I1  miss rate:    0.26%
LLi miss rate:    0.25%
D1  miss rate:     2.3%
LLd miss rate:     2.1%
LL miss rate:      0.8%
Mispred rate:      5.7% 

** O1 optimization **
I1  miss rate:    0.41%
LLi miss rate:    0.41%
D1  miss rate:     4.9%
LLd miss rate:     4.5%
LL miss rate:      1.4% 
Mispred rate:      5.8% 

** O2 optimization **
I1  miss rate:    0.41%
LLi miss rate:    0.41%
D1  miss rate:     4.9% 
LLd miss rate:     4.5%
LL miss rate:      1.4% 
Mispred rate:      5.7%

** O3 optimization **
I1  miss rate:    0.59%
LLi miss rate:    0.58%
D1  miss rate:     6.3%
LLd miss rate:     5.7% 
LL miss rate:      2.0%
Mispred rate:      8.8%

[ j ] [ i ] setup

** O0 optimization **
I1  miss rate:    0.26%
LLi miss rate:    0.25%
D1  miss rate:    14.8% 
LLd miss rate:     2.1%
LL miss rate:      0.8%
Mispred rate:      5.7%

** O1 optimization **
I1  miss rate:    0.41%
LLi miss rate:    0.41%
D1  miss rate:    32.0% 
LLd miss rate:     4.5%
LL miss rate:      1.4%
Mispred rate:      5.8%

** O2 optimization **
I1  miss rate:    0.41%
LLi miss rate:    0.41%
D1  miss rate:    32.0%
LLd miss rate:     4.5%
LL miss rate:      1.4% 
Mispred rate:      5.8%

** O3 optimization **
I1  miss rate:    0.59%
LLi miss rate:    0.58%
D1  miss rate:    13.2% 
LLd miss rate:     5.7%
LL miss rate:      2.0%
Mispred rate:      8.5%

Between the two matrix implementations we see that the misprediction 
rate is fairly the same, with rates of 8.5% for Level 3 optimization,
5.8% with O1 and O2 levels of optimization and 5.7 for level 0 
optimization. Again, we see that given the small size of the matrix, 
optimization doesn’t have differing effects on misprediction 
rates between the 2 implementations. We see that an optimization 
from L0 to L1 has the misprediction rate at the same rate for both 
implementations and the L1, LLi, LLd, and LL miss rates all react 
the same way to optimization across matrix implementations. 
Interestingly, the D1 miss rate differs in its behavior across the 
two matrix implementations; in the [ i ][ j ] , -00 setup, the D1 
miss rate is low, at 2.3% while at 14.8% in the slower implementation. 
Increasing optimization, the [i][j], -01, -02, and -03 setups show 
D1 miss rates of 4.9%, 4.9%, and 6.3% while for [ j ][ i ], -01, 
-02, and -03 setups we get 32%, 32%, and 13.2%, so clearly the 
optimization actually increases D1 misprediction rates from -00 
to -01 but time stays constant despite this. In general though, 
of the 5 cache statistics, 4 / 5 are consistent regardless of 
the matrix implementation, likely because the size is small. 
When the size is far larger, a size of 4096, we get markedly 
different results. Specifically, the [ i ] [ j ] matrix 
implementation gives results: 

O0 optimization: 0.032u 0.016s 0:00.04 100.0%	0+0k 0+0io 0pf+0w
O1 optimization: 0.006u 0.014s 0:00.02 50.0%	0+0k 0+0io 0pf+0w
O2 optimization: 0.006u 0.014s 0:00.02 50.0%	0+0k 0+0io 0pf+0w
O3 optimization: 0.002u 0.015s 0:00.01 100.0%	0+0k 0+0io 0pf+0w

While the  [ j ] [ i ] setup produces the following results at 
size 128: 

O0 optimization: 0.232u 0.014s 0:00.24 100.0%	0+0k 0+0io 0pf+0w
O1 optimization: 0.144u 0.016s 0:00.16 93.7%	0+0k 0+0io 0pf+0w
O2 optimization: 0.148u 0.015s 0:00.16 93.7%	0+0k 0+0io 0pf+0w
O3 optimization: 0.041u 0.017s 0:00.05 100.0%	0+0k 0+0io 0pf+0w

We see that for the faster matrix implementation. The 01 optimization 
reduced running time the most, marking an 81.25% decrease in runtime 
between -00 and -01. For the slower matrix implementation this jump 
from 00 to 01 only represents a 37.93% decrease in runtime. While 
the 02 to 03 optimization produced a marginal improvement in running 
time for the faster implementation, it represented a 72.29% 
decrease in runtime for the slower matrix implementation, with a 
decrease in runtime from .148u to .041u. Thus, the optimizations 
have different efficacy levels depending on one’s program. If one 
is running a [j ] [i ] implementation, they’ll see the steepest 
percent change in runtime from 02 to 03 whereas running [i ] [j] 
sees the steepest drop in percent change in the 00 to 01 optimization. 
In general though, optimization does surely reduce runtime when 
the matrix size is larger, like in the case of size 
4096. At this size, the cache preformance of matrix  [i ] [j] 
is as follows: 


****************** SIZE 4096 [i][j] **************

** O0 optimization **

0.032u 0.016s 0:00.04 100.0%	0+0k 0+0io 0pf+0w

 I   refs:      167,898,754
 I1  misses:            673
 LLi misses:            667
 I1  miss rate:        0.00%
 LLi miss rate:        0.00%
 D1  miss rate:         1.3% (       0.0%     +        6.2%  )
 LLd miss rate:         1.3% (       0.0%     +        6.2%  )
 LL miss rate:          0.4% (       0.0%     +        6.2%  )
 Mispred rate:          0.0% (       0.0%     +       27.4%   )

** O1 optimization **
 
 I1  miss rate:       0.00%
 LLi miss rate:       0.00%
 D1  miss rate:        6.2% (       4.8%     +        6.2%  )
 LLd miss rate:        6.2% (       4.2%     +        6.2%  )
 LL miss rate:         1.3% (       0.0%     +        6.2%  )
 Mispred rate:         0.0% (       0.0%     +       27.4%   )

** O2 optimization **

 I1  miss rate:       0.00%
 LLi miss rate:       0.00%
 D1  miss rate:        6.2% (       4.8%     +        6.2%  )
 LLd miss rate:        6.2% (       4.2%     +        6.2%  )
 LL miss rate:         1.3% (       0.0%     +        6.2%  )
 Mispred rate:         0.0% (       0.0%     +       27.4%   )

** O3 optimization **

 I1  miss rate:       0.00%
 LLi miss rate:       0.00%
D1  miss rate:       24.8% (      4.8%     +      24.9%  )
LLd miss rate:       24.8% (      4.2%     +      24.9%  )
LL miss rate:         5.0% (      0.0%     +      24.9%  )
Mispred rate:         0.1% (      0.1%     +      27.4%   )

While the cache preformance of matrix [j][i] is as follows: 

** O0 optimization **

I1  miss rate:        0.00%
LLi miss rate:        0.00%
D1  miss rate:        20.0% (       0.0%     +       99.9%  )
LLd miss rate:        20.0% (       0.0%     +       99.9%  )
LL miss rate:          6.7% (       0.0%     +       99.9%  )
Mispred rate:          0.0% (       0.0%     +       27.4%   )

** O1 optimization **

I1  miss rate:       0.00%
LLi miss rate:       0.00%
D1  miss rate:       99.8% (       4.8%     +       99.9%  )
LLd miss rate:       99.8% (       4.2%     +       99.9%  )
LL miss rate:        20.0% (       0.0%     +       99.9%  )
Mispred rate:         0.0% (       0.0%     +       27.4%   )

** O2 optimization **

I1  miss rate:       0.00%
LLi miss rate:       0.00%
D1  miss rate:       99.8% (       4.8%     +       99.9%  )
LLd miss rate:       99.8% (       4.2%     +       99.9%  )
LL miss rate:        20.0% (       0.0%     +       99.9%  )
Mispred rate:         0.0% (       0.0%     +       27.4%   )

** O3 optimization **

I1  miss rate:       0.00%
LLi miss rate:       0.00%
D1  miss rate:       99.1% (      4.8%     +      99.7%  )
LLd miss rate:       99.1% (      4.2%     +      99.7%  )
LL miss rate:        19.9% (      0.0%     +      99.7%  )
Mispred rate:         0.1% (      0.1%     +      27.4%   )

For the [j][i] setup, the misprediction rate remains constant as 
optimization increases (00, 01, 02, 03), around 0. As we move from 
00 to 01, the LL misprediction rate moves from 6.7% to 20 % and 
remains in that range as we continue optimizing to higher levels 
while the D1 jumps from 20% to 99% from 00 to 01. As such, we see 
optimization levels produce very different miss rates at different 
levels of caches. For the [i][j] setup, the misprediction rate 
and I1 and LLi miss rates remain constant as optimization 
increases (00, 01, 02, 03), around 0. However, we see that the D1 
miss rate increases from 1.3% to 6.2% from 00 to 01 
(and stays at 6.2% from 01 to 02) and jumps up to 24.8% from 02 
optimization level to 03. Again, optimization levels produce 
very different miss rates at different levels of caches. 


*********************************************
		PROBLEM 2

1. Language: Java

2. File name: CacheSimulator.java

3. How to compile:
	> javac CacheSimulator.java

4. How to run:
	For instance, to simulate a cache with 256 sets of 4 blocks each,
	with each block containing 16 bytes of memory; the cache performs
	write-allocate but no write-through, and it abides to LRU, run: 
		> java CacheSimulator 256 4 16 1 0 1 gcc.trace

Thank you!
*********************************************